{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a4f447",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec32b0",
   "metadata": {},
   "source": [
    "* 테크니컬한 내용들을 설명...\n",
    "* ppt 대충 필요하긴 할듯...\n",
    "* 어카냐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966a676",
   "metadata": {},
   "source": [
    "## 2025-10-20 HFRL\n",
    "\n",
    "* DPO: Direct Preference Optimization\n",
    "> 로그 확률의 차이가 사람의 선호도를 잘 추적하도록 만듦\n",
    "* LLM + RLHF + Survival Analysis: C-index 개선 알고리즘 태스크가 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe352c8",
   "metadata": {},
   "source": [
    "`-` Datasets\n",
    "\n",
    "* PhysioNet\n",
    "> MIMIC-IV, elCU CRD, HiRID 3가지\n",
    "* Amsterdam Medical Data Science\n",
    "\n",
    "> 사망/생존 여부와 입원 일자, 퇴원 여부, 여러 수치 입력되어 있음.\n",
    ">\n",
    "> survival time, censoring indicator, attributes 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470796d",
   "metadata": {},
   "source": [
    "`-` Discussion Points\n",
    "\n",
    "* 모든 변수들을 LLM에 포함? $\\to$ 일단 다 쓰면 좋죠. 일단 다 던져주고, LLM이 자율적으로 하도록 두는 게 좋지 않을까?\n",
    "* 어떤 LLM 아키텍쳐 사용? $\\to$ 라마 써야죠 뭐.\n",
    "* C-index가 잘 나올만한 언어? 레이블링을 어떻게 해야 하나? $\\to$ survival time이 아닌 label이 뭐인지 규명?\n",
    "* DPO에서 선호도 규명이 요구됨 $\\to$ 사람이 해야 함. 전문가가 해야돼요.\n",
    "* 모델에서 바로 C-index를 예측하는 end-to-end? 또는 임베디드 텍스트 생성 후 다른 딥러닝 모델에 삽입?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02423006",
   "metadata": {},
   "source": [
    "`-` 질문?\n",
    "\n",
    "* 현재 상황은 Tabular data만 사용할 수 있는 상황인가요?\n",
    "> 비정형 데이터를 같이 다루려고 출발했는데\n",
    "\n",
    "* 유연한 모델, LLM에게 넘어가는 단계를 시켜서\n",
    "> SFT + DPO. 어떤 생성을 선호한다고 입력하여 교정. SFT와 DPO의 label이 약간 달라도 괜찮을 것이라고 생각됨. SFT는 기본 예열이고, DPO로 preference를 학습. 원하는 형태의 language로 잘 바뀌도록 설계하는 것이 목적임 ㅇㅇ\n",
    "\n",
    "* 단조로운 형태가 아닌, 복잡한 형태가 될 것임. SFT는 학습의 문제, DPO에서는 디자인을 어떻게 할지\n",
    "> SFT, DPO 어떻게 하는지. Code Level에서의 문제\n",
    ">\n",
    "> DPO는 할 수 있는 부분만 일단은..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795874b8",
   "metadata": {},
   "source": [
    "Reinforcement Learning with Reward Rectification -> DFT loss function을 사용해야 한다는데, 여기서도 적용되는 건가?\n",
    "\n",
    "`loss_type=dft`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523e960",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
